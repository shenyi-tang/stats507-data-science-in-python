{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sentiment Analysis for Reviews from 20 Apps",
   "id": "94eab9e1417c24e1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T21:35:28.410586Z",
     "start_time": "2024-12-06T21:35:28.405490Z"
    }
   },
   "source": [
    "from encodings import search_function\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T18:23:01.289112Z",
     "start_time": "2024-12-06T18:23:01.095684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read data\n",
    "data_path = \"data/all_combined.csv\"\n",
    "odf = pd.read_csv(data_path, encoding='utf-8', on_bad_lines=\"skip\")"
   ],
   "id": "febf1efc8d58ab30",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Clean Data and Establish new tag\n",
    "1. To classify the reviews into 3 groups: negative, neutral and positive, I use a new mapping rule to create a new field called `flag`\n",
    "2. Create a new flag to indicate whether the content contains emojis. It's unknown that whether the existence of emojis will affect the performance of the model\n",
    "3. Drop some non-English reviews\n",
    "4. Drop blank data"
   ],
   "id": "85dca54b6549b4fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:31:32.212282Z",
     "start_time": "2024-12-06T19:31:32.203501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create new field to re-classify the score\n",
    "score_to_flag = {1:-1, 2:-1, 3:0, 4:1, 5:1}\n",
    "odf['flag'] = odf['score'].map(score_to_flag)"
   ],
   "id": "e445f2cd029c63b6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:57:09.682092Z",
     "start_time": "2024-12-06T19:57:09.250379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to show if the content includes emoji\n",
    "def contains_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "odf[\"has_emoji\"] = odf[\"content\"].apply(contains_emoji)"
   ],
   "id": "519eb17363d7375a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:45:21.259114Z",
     "start_time": "2024-12-06T20:45:20.362009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter whose score is not in 1-5\n",
    "new_df = odf[odf[\"score\"].between(1, 5, inclusive=\"both\")]\n",
    "\n",
    "# strip out of non-ascii char or non-emoji part in the content\n",
    "def clean_review_content(text):\n",
    "    \"\"\"\n",
    "    Clean review content by:\n",
    "    1. Preserving emojis\n",
    "    2. Removing non-ASCII characters except emojis\n",
    "    3. Stripping extra whitespaces\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input review text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned review text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''\n",
    "    try:\n",
    "        # Extract emojis and save them\n",
    "        emojis = ''.join(c for c in text if c in emoji.EMOJI_DATA)\n",
    "        \n",
    "        # Remove non-ASCII characters, keeping emojis\n",
    "        cleaned_text = ''.join(c for c in text if (ord(c) < 128) or (c in emoji.EMOJI_DATA))\n",
    "        \n",
    "        # Remove multiple whitespaces and trim\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        \n",
    "        # Combine cleaned text with preserved emojis\n",
    "        return (cleaned_text + ' ' + emojis).strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "new_df[\"cleaned_review\"] = new_df[\"content\"].apply(clean_review_content)\n"
   ],
   "id": "30c5a7c33b238e54",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:32:19.581997Z",
     "start_time": "2024-12-07T01:32:19.500463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop NULL data\n",
    "new_df = new_df.dropna(subset=[\"cleaned_review\", \"score\", \"content\"])\n",
    "new_df = new_df[new_df[\"cleaned_review\"] != \"\"]\n",
    "print(f\"The original data size: {odf.shape}\")\n",
    "print(f\"The cleaned data size: {new_df.shape}\")"
   ],
   "id": "80826fc256cd9b0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data size: (200000, 6)\n",
      "The cleaned data size: (196688, 7)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:32:25.062687Z",
     "start_time": "2024-12-07T01:32:25.053937Z"
    }
   },
   "cell_type": "code",
   "source": "new_df.head(20)",
   "id": "6cb1c3b5d2f23822",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                reviewId  \\\n",
       "0   e2996bb1-cdf1-4f76-a4e5-88d47b3b8d5e   \n",
       "1   d32653e0-f81f-43d5-8d61-e0d5ce419eb0   \n",
       "2   6a4cd47e-44ee-4bfd-b1f7-f0c044e8419e   \n",
       "3   d8e578db-d679-4fd6-ab11-cef028134049   \n",
       "4   f6b1b7d5-5028-42c8-bb30-b57c6bf1a02b   \n",
       "5   3130b4cf-53b7-4823-8669-a44e52e3a3d5   \n",
       "6   9b870115-9d74-4d97-bcc3-37ae9dd0a1a2   \n",
       "7   0f45902f-ed8b-4a8a-a0d5-dda4e06d30b2   \n",
       "8   88e523fa-e2f4-4ad1-a40b-4315dcf56345   \n",
       "9   3aa36269-df4a-4eb3-919a-c92d45d10af5   \n",
       "10  1d7f9eec-726a-44fd-ae1d-f96456113d09   \n",
       "11  e562a0d4-6ede-4210-bf9b-8d6de7ce2fa6   \n",
       "13  456ac811-fff2-404d-bcb1-66df63e99966   \n",
       "14  3c34abfd-9f32-470e-b961-c91be564896d   \n",
       "15  9ab88748-7f9b-47fb-a53d-135ac917ba67   \n",
       "16  d7bc41fe-3d08-49f3-9e62-e348f570f052   \n",
       "17  f741ff16-19e2-4ae3-b1f4-921079a5e829   \n",
       "18  ed537838-524e-4b22-8760-54112723c391   \n",
       "19  bd801db6-96ff-4145-a67c-7ea49f66b848   \n",
       "20  29977c65-264e-4a10-8130-8aa97c7d5791   \n",
       "\n",
       "                                              content  score       app  flag  \\\n",
       "0                                                 Oop      5  Facebook     1   \n",
       "1                              Facebook is a nice app      5  Facebook     1   \n",
       "2                                                best      5  Facebook     1   \n",
       "3                                Open Facebook update      5  Facebook     1   \n",
       "4   Facebook bhoot aacha chize hai aap sabhi log b...      5  Facebook     1   \n",
       "5                                            Nice 💯👍🫦      5  Facebook     1   \n",
       "6            Too many annoying useless notifications.      1  Facebook    -1   \n",
       "7                      Soferrr gandaaa Siya sistaa!!!      5  Facebook     1   \n",
       "8                                            Best 👌 👍      4  Facebook     1   \n",
       "9                                                nice      5  Facebook     1   \n",
       "10                                           So crazy      1  Facebook    -1   \n",
       "11                                              🔥🔥🔥🥰🥰      5  Facebook     1   \n",
       "13                                               Nice      5  Facebook     1   \n",
       "14                                      hii fb family      5  Facebook     1   \n",
       "15                                           Good app      5  Facebook     1   \n",
       "16                                                  😞      5  Facebook     1   \n",
       "17                                        very good 👍      5  Facebook     1   \n",
       "18                                        very good 👍      5  Facebook     1   \n",
       "19                                               good      5  Facebook     1   \n",
       "20                                          Great day      5  Facebook     1   \n",
       "\n",
       "    has_emoji                                     cleaned_review  \n",
       "0       False                                                Oop  \n",
       "1       False                             Facebook is a nice app  \n",
       "2       False                                               best  \n",
       "3       False                               Open Facebook update  \n",
       "4        True  Facebook bhoot aacha chize hai aap sabhi log b...  \n",
       "5        True                                       Nice 💯👍🫦 💯👍🫦  \n",
       "6       False           Too many annoying useless notifications.  \n",
       "7       False                     Soferrr gandaaa Siya sistaa!!!  \n",
       "8        True                                        Best 👌 👍 👌👍  \n",
       "9       False                                               nice  \n",
       "10      False                                           So crazy  \n",
       "11       True                                        🔥🔥🔥🥰🥰 🔥🔥🔥🥰🥰  \n",
       "13      False                                               Nice  \n",
       "14      False                                      hii fb family  \n",
       "15      False                                           Good app  \n",
       "16       True                                                😞 😞  \n",
       "17       True                                      very good 👍 👍  \n",
       "18       True                                      very good 👍 👍  \n",
       "19      False                                               good  \n",
       "20      False                                          Great day  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>app</th>\n",
       "      <th>flag</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e2996bb1-cdf1-4f76-a4e5-88d47b3b8d5e</td>\n",
       "      <td>Oop</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Oop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d32653e0-f81f-43d5-8d61-e0d5ce419eb0</td>\n",
       "      <td>Facebook is a nice app</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Facebook is a nice app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6a4cd47e-44ee-4bfd-b1f7-f0c044e8419e</td>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d8e578db-d679-4fd6-ab11-cef028134049</td>\n",
       "      <td>Open Facebook update</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Open Facebook update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f6b1b7d5-5028-42c8-bb30-b57c6bf1a02b</td>\n",
       "      <td>Facebook bhoot aacha chize hai aap sabhi log b...</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook bhoot aacha chize hai aap sabhi log b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3130b4cf-53b7-4823-8669-a44e52e3a3d5</td>\n",
       "      <td>Nice 💯👍🫦</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Nice 💯👍🫦 💯👍🫦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9b870115-9d74-4d97-bcc3-37ae9dd0a1a2</td>\n",
       "      <td>Too many annoying useless notifications.</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Too many annoying useless notifications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0f45902f-ed8b-4a8a-a0d5-dda4e06d30b2</td>\n",
       "      <td>Soferrr gandaaa Siya sistaa!!!</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Soferrr gandaaa Siya sistaa!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88e523fa-e2f4-4ad1-a40b-4315dcf56345</td>\n",
       "      <td>Best 👌 👍</td>\n",
       "      <td>4</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Best 👌 👍 👌👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3aa36269-df4a-4eb3-919a-c92d45d10af5</td>\n",
       "      <td>nice</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1d7f9eec-726a-44fd-ae1d-f96456113d09</td>\n",
       "      <td>So crazy</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>So crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e562a0d4-6ede-4210-bf9b-8d6de7ce2fa6</td>\n",
       "      <td>🔥🔥🔥🥰🥰</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>🔥🔥🔥🥰🥰 🔥🔥🔥🥰🥰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>456ac811-fff2-404d-bcb1-66df63e99966</td>\n",
       "      <td>Nice</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3c34abfd-9f32-470e-b961-c91be564896d</td>\n",
       "      <td>hii fb family</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>hii fb family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9ab88748-7f9b-47fb-a53d-135ac917ba67</td>\n",
       "      <td>Good app</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Good app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d7bc41fe-3d08-49f3-9e62-e348f570f052</td>\n",
       "      <td>😞</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>😞 😞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>f741ff16-19e2-4ae3-b1f4-921079a5e829</td>\n",
       "      <td>very good 👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>very good 👍 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ed537838-524e-4b22-8760-54112723c391</td>\n",
       "      <td>very good 👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>very good 👍 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bd801db6-96ff-4145-a67c-7ea49f66b848</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29977c65-264e-4a10-8130-8aa97c7d5791</td>\n",
       "      <td>Great day</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Great day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Modelling",
   "id": "853e148aafbea446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T05:25:49.306048Z",
     "start_time": "2024-12-07T05:25:49.297009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import necessary packages\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "# natural language tool kits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "465ee52d859d2522",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T05:34:47.434318Z",
     "start_time": "2024-12-07T05:34:47.424180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# download necessary NLTK resources\n",
    "# stopwords: words such as \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "## set random seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "id": "6dcfa96ee085f309",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:44:21.630859Z",
     "start_time": "2024-12-07T07:44:21.627190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the dataset with reviews, features and labels.\n",
    "    \n",
    "    \n",
    "    :param texts: List/array of review texts\n",
    "    :param app_features: List/array of feature vectors for each app\n",
    "    :param lables: List/array of corresponding labels/ratings\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, app_features, lables):\n",
    "        self.texts = texts\n",
    "        self.app_features = app_features\n",
    "        self.lables = lables\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the total number of samples in the dataset.\n",
    "    Required by PyTorch Dataset class.\n",
    "    \n",
    "    :returns int: Number of reviews/samples in the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch a single sample from the dataset at the specified index.\n",
    "    Required by PyTorch Dataset class.\n",
    "    \n",
    "    :param idx: Index of the sample to retrieve\n",
    "    :returns tuple: (review_text, app_features, label) for the specified index\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.app_features[idx], self.lables[idx]"
   ],
   "id": "6158251f8976af",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T05:34:49.916596Z",
     "start_time": "2024-12-07T05:34:49.911666Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 69,
   "source": [
    "# create LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, hidden_dim, n_layers, dropout, app_feature_dim):\n",
    "        \"\"\"\n",
    "        Initialize LSTM Model for Sentiment Classification with App Features\n",
    "        \n",
    "        :param vocab_size (int): Total number of unique words in vocabulary\n",
    "        :param embedding_dim (int): Size of word embedding vectors\n",
    "        :param output_dim (int): Number of output classes (e.g., 2 for binary sentiment)\n",
    "        :param hidden_dim (int): Number of features in LSTM hidden state\n",
    "        :param n_layers (int): Number of stacked LSTM layers\n",
    "        :param dropout (float): Dropout rate for regularization\n",
    "        :param app_feature_dim (int): Dimension of additional app-specific features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Create embedding layer to convert word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer processes sequence of word embeddings\n",
    "        # batch_first=True means input shape is (batch_size, sequence_length, features)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=dropout if n_layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # First fully connected layer combines LSTM output with app features\n",
    "        self.fc1 = nn.Linear(hidden_dim + app_feature_dim, 128)\n",
    "        \n",
    "        # Output layer produces final classification\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        \n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ReLU activation function adds non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, text, app_features):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        \n",
    "        :param text (torch.Tensor): Input tensor of word indices, shape (batch_size, sequence_length)\n",
    "        :param app_features (torch.Tensor): Additional app-specific features, shape (batch_size, app_feature_dim)\n",
    "        \n",
    "        \n",
    "        :returns torch.Tensor: Model predictions, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings and apply dropout\n",
    "        embedded = self.dropout(self.embedding(text)) # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Process sequence through LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden state from last LSTM layer\n",
    "        hidden = self.dropout(hidden[-1, :, :])\n",
    "        # Concatenate hidden state with app features\n",
    "        combined = torch.cat([hidden, app_features], dim=1) # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Pass through fully connected layers with dropout and ReLU\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Return final predictions\n",
    "        return self.fc2(x)"
   ],
   "id": "43f8bd9b04083967"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T05:34:58.131707Z",
     "start_time": "2024-12-07T05:34:58.115690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentAnalyzer:\n",
    "    \n",
    "    \"\"\"\n",
    "    A comprehensive sentiment analysis pipeline that processes app reviews\n",
    "    Combines text processing, LSTM modeling, and app-specific features\n",
    "    \n",
    "    Key Features:\n",
    "    - Text preprocessing with emoji preservation\n",
    "    - Custom vocabulary building\n",
    "    - LSTM-based sentiment classification\n",
    "    - Integration of app-specific features\n",
    "    - Built-in training and evaluation pipeline\n",
    "    \n",
    "    :param df: DataFrame containing review data\n",
    "    :param content_col: Name of column containing review text\n",
    "    :param label_col: Name of column containing sentiment labels\n",
    "    :param app_col: Name of column containing app identifiers\n",
    "    :param test_size: Proportion of data to use for testing (0-1)\n",
    "    :param random_state: Random seed for reproducible results\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, content_col: str = \"cleaned_review\",\n",
    "                 label_col: str = \"flag\", app_col: str = \"app\", \n",
    "                 test_size: float = 0.2, random_state: int = 42):\n",
    "        \n",
    "        # Create a copy of input data to avoid modifications to original\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # Preprocess all review texts\n",
    "        self.df['processed_text'] = self.df[content_col].apply(self.preprocess_text)\n",
    "        \n",
    "        # Build vocabulary from processed texts\n",
    "        self.vocab = self.build_vocab(self.df['processed_text'])\n",
    "        \n",
    "        # Create word-to-index mapping with special tokens\n",
    "        self.word_to_idx = {word: i + 1 for i, word in enumerate(self.vocab)}\n",
    "        self.word_to_idx['<PAD>'] = 0  \n",
    "        self.word_to_idx['<UNK>'] = len(self.word_to_idx)\n",
    "        \n",
    "        # Convert texts to numerical sequences\n",
    "        self.encoded_texts = self.encode_texts(self.df['processed_text'])\n",
    "        \n",
    "        # Encode sentiment labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(label_encoder.fit_transform(self.df[label_col]))\n",
    "        \n",
    "        # One-hot encode app features\n",
    "        app_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.app_features = torch.tensor(\n",
    "            app_encoder.fit_transform(self.df[app_col].values.reshape(-1, 1)),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        self.X_train, self.X_test, self.app_train, self.app_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.encoded_texts,\n",
    "            self.app_features,\n",
    "            self.labels,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=self.labels\n",
    "        )\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        self.train_dataset = ReviewDataset(self.X_train, self.app_train, self.y_train)\n",
    "        self.test_dataset = ReviewDataset(self.X_test, self.app_test, self.y_test)\n",
    "        \n",
    "        # Define model hyperparameters\n",
    "        self.model_params = {\n",
    "            'vocab_size': len(self.word_to_idx),\n",
    "            'embedding_dim': 100,\n",
    "            'hidden_dim': 128,\n",
    "            'output_dim': len(np.unique(self.labels)),\n",
    "            'n_layers': 2,\n",
    "            'dropout': 0.5,\n",
    "            'app_feature_dim': self.app_features.shape[1]\n",
    "        }\n",
    "        \n",
    "       # Initialize model, loss function, and optimizer\n",
    "        self.model = SentimentLSTM(**self.model_params)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "    \n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text while preserving emojis\n",
    "        \n",
    "        Steps:\n",
    "        1. Convert to lowercase\n",
    "        2. Tokenize while keeping emojis intact\n",
    "        3. Remove stopwords (except emojis)\n",
    "        4. Join tokens back into text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        def tokenize_with_emojis(text):\n",
    "            \"\"\"\n",
    "            Custom tokenizer that preserves emojis as distinct tokens\n",
    "            Handles the text character by character to properly separate\n",
    "            emojis from regular words\n",
    "            \"\"\"\n",
    "            tokens = []\n",
    "            current_token = []\n",
    "            \n",
    "            for char in text:\n",
    "                if emoji.is_emoji(char):\n",
    "                    # If there's a current token, save it\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "                    # Add emoji as a separate token\n",
    "                    tokens.append(char)\n",
    "                elif char.isalnum():\n",
    "                    current_token.append(char)\n",
    "                else:\n",
    "                    # If there's a current token, save it\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "            \n",
    "            # Handle any remaining token\n",
    "            if current_token:\n",
    "                tokens.append(''.join(current_token))\n",
    "            \n",
    "            return tokens\n",
    "        \n",
    "        # Tokenize and remove stopwords (preserve emojis)\n",
    "        tokens = tokenize_with_emojis(text)  \n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [token for token in tokens if token not in stop_words or emoji.is_emoji(token)]\n",
    "        return ' '.join(tokens) \n",
    "    \n",
    "    def build_vocab(self, texts: pd.Series, max_vocab_size: int = 10000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from texts, including emojis\n",
    "        \n",
    "        \n",
    "        :param texts (pd.Series): Series of processed texts\n",
    "        :param max_vocab_size (int): Maximum vocabulary size\n",
    "        \n",
    "        \n",
    "        :returns List[str]: Vocabulary words and emojis\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for token in text.split():\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "        \n",
    "        # Sort by frequency and limit vocabulary\n",
    "        vocab = sorted(word_freq, key=word_freq.get, reverse=True)[:max_vocab_size]\n",
    "        return vocab\n",
    "    \n",
    "    def encode_texts(self, texts: pd.Series, max_length: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode texts to tensor, handling emojis\n",
    "        \n",
    "        :param texts (pd.Series): Series of processed texts\n",
    "        :param max_length (int): Maximum sequence length\n",
    "        \n",
    "        :returns torch.Tensor: Encoded texts\n",
    "        \"\"\"\n",
    "        encoded_texts = []\n",
    "        for text in texts:\n",
    "            # Convert tokens to indices\n",
    "            indices = [\n",
    "                self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
    "                for token in text.split()\n",
    "            ]\n",
    "            \n",
    "            # Pad or truncate to fixed length\n",
    "            if len(indices) > max_length:\n",
    "                indices = indices[:max_length]\n",
    "            else:\n",
    "                indices = indices + [self.word_to_idx['<PAD>']] * (max_length - len(indices))\n",
    "            \n",
    "            encoded_texts.append(indices)\n",
    "        \n",
    "        return torch.tensor(encoded_texts)\n",
    "    \n",
    "    def train(self, epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Train the LSTM model\n",
    "        \n",
    "        \n",
    "        :param epochs (int): Number of training epochs\n",
    "        :param batch_size (int): Batch size for training\n",
    "        \"\"\"\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_total_loss = 0\n",
    "            \n",
    "            for texts, app_features, labels in train_loader:\n",
    "                # Clear gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(texts, app_features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_total_loss += loss.item()\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            avg_train_loss = train_total_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            test_total_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for texts, app_features, labels in test_loader:\n",
    "                    outputs = self.model(texts, app_features)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    test_total_loss += loss.item()\n",
    "            \n",
    "            # Calculate average test loss\n",
    "            avg_test_loss = test_total_loss / len(test_loader)\n",
    "            \n",
    "            # Print both losses\n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Test Loss: {avg_test_loss:.4f}\\n')\n",
    "            \n",
    "    def evaluate(self) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "        \n",
    "       \n",
    "        :returns dict: Performance metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, app_features, labels in test_loader:\n",
    "                outputs = self.model(texts, app_features)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
    "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
    "            'f1_score': f1_score(all_labels, all_preds, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "            'classification_report': classification_report(all_labels, all_preds)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "            "
   ],
   "id": "b38b37b741b5820b",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T06:06:54.390961Z",
     "start_time": "2024-12-07T05:35:07.419705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "        set_seed(507)\n",
    "        # Initialize Sentiment Analyzer\n",
    "        analyzer = SentimentAnalyzer(new_df)\n",
    "        \n",
    "        # Train the model\n",
    "        analyzer.train(epochs=10)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = analyzer.evaluate()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "1d1e4596d77eab35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 0.7297\n",
      "Test Loss: 0.7190\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.7216\n",
      "Test Loss: 0.7185\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.7208\n",
      "Test Loss: 0.7184\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.7201\n",
      "Test Loss: 0.7178\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.7197\n",
      "Test Loss: 0.7183\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.6469\n",
      "Test Loss: 0.5410\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.5349\n",
      "Test Loss: 0.5071\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.5115\n",
      "Test Loss: 0.4948\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.4992\n",
      "Test Loss: 0.4892\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.4900\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Model Performance Metrics:\n",
      "accuracy:\n",
      "0.8352483603640246\n",
      "\n",
      "precision:\n",
      "0.7878294310216243\n",
      "\n",
      "recall:\n",
      "0.8352483603640246\n",
      "\n",
      "f1_score:\n",
      "0.8099420265903265\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 7314     0  2867]\n",
      " [  752     0  1355]\n",
      " [ 1507     0 25543]]\n",
      "\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74     10181\n",
      "           1       0.00      0.00      0.00      2107\n",
      "           2       0.86      0.94      0.90     27050\n",
      "\n",
      "    accuracy                           0.84     39338\n",
      "   macro avg       0.54      0.55      0.55     39338\n",
      "weighted avg       0.79      0.84      0.81     39338\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:44:21.587198Z",
     "start_time": "2024-12-07T06:09:39.795368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# increase epochs\n",
    "def main():\n",
    "        set_seed(507)\n",
    "        # Initialize Sentiment Analyzer\n",
    "        analyzer = SentimentAnalyzer(new_df)\n",
    "        \n",
    "        # Train the model\n",
    "        analyzer.train(epochs=30)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = analyzer.evaluate()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "ed3ff38e4e7c6c05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "Training Loss: 0.7297\n",
      "Test Loss: 0.7190\n",
      "\n",
      "Epoch 2/30:\n",
      "Training Loss: 0.7216\n",
      "Test Loss: 0.7185\n",
      "\n",
      "Epoch 3/30:\n",
      "Training Loss: 0.7208\n",
      "Test Loss: 0.7184\n",
      "\n",
      "Epoch 4/30:\n",
      "Training Loss: 0.7201\n",
      "Test Loss: 0.7178\n",
      "\n",
      "Epoch 5/30:\n",
      "Training Loss: 0.7197\n",
      "Test Loss: 0.7183\n",
      "\n",
      "Epoch 6/30:\n",
      "Training Loss: 0.6469\n",
      "Test Loss: 0.5410\n",
      "\n",
      "Epoch 7/30:\n",
      "Training Loss: 0.5349\n",
      "Test Loss: 0.5071\n",
      "\n",
      "Epoch 8/30:\n",
      "Training Loss: 0.5115\n",
      "Test Loss: 0.4948\n",
      "\n",
      "Epoch 9/30:\n",
      "Training Loss: 0.4992\n",
      "Test Loss: 0.4892\n",
      "\n",
      "Epoch 10/30:\n",
      "Training Loss: 0.4900\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Epoch 11/30:\n",
      "Training Loss: 0.4838\n",
      "Test Loss: 0.4893\n",
      "\n",
      "Epoch 12/30:\n",
      "Training Loss: 0.4800\n",
      "Test Loss: 0.4833\n",
      "\n",
      "Epoch 13/30:\n",
      "Training Loss: 0.4758\n",
      "Test Loss: 0.4875\n",
      "\n",
      "Epoch 14/30:\n",
      "Training Loss: 0.4713\n",
      "Test Loss: 0.4840\n",
      "\n",
      "Epoch 15/30:\n",
      "Training Loss: 0.4689\n",
      "Test Loss: 0.4911\n",
      "\n",
      "Epoch 16/30:\n",
      "Training Loss: 0.4666\n",
      "Test Loss: 0.4861\n",
      "\n",
      "Epoch 17/30:\n",
      "Training Loss: 0.4641\n",
      "Test Loss: 0.4848\n",
      "\n",
      "Epoch 18/30:\n",
      "Training Loss: 0.4623\n",
      "Test Loss: 0.4887\n",
      "\n",
      "Epoch 19/30:\n",
      "Training Loss: 0.4599\n",
      "Test Loss: 0.4826\n",
      "\n",
      "Epoch 20/30:\n",
      "Training Loss: 0.4588\n",
      "Test Loss: 0.4823\n",
      "\n",
      "Epoch 21/30:\n",
      "Training Loss: 0.4568\n",
      "Test Loss: 0.5100\n",
      "\n",
      "Epoch 22/30:\n",
      "Training Loss: 0.4548\n",
      "Test Loss: 0.4867\n",
      "\n",
      "Epoch 23/30:\n",
      "Training Loss: 0.4522\n",
      "Test Loss: 0.4895\n",
      "\n",
      "Epoch 24/30:\n",
      "Training Loss: 0.4529\n",
      "Test Loss: 0.4857\n",
      "\n",
      "Epoch 25/30:\n",
      "Training Loss: 0.4513\n",
      "Test Loss: 0.5014\n",
      "\n",
      "Epoch 26/30:\n",
      "Training Loss: 0.4502\n",
      "Test Loss: 0.4976\n",
      "\n",
      "Epoch 27/30:\n",
      "Training Loss: 0.4485\n",
      "Test Loss: 0.5266\n",
      "\n",
      "Epoch 28/30:\n",
      "Training Loss: 0.4474\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Epoch 29/30:\n",
      "Training Loss: 0.4465\n",
      "Test Loss: 0.4918\n",
      "\n",
      "Epoch 30/30:\n",
      "Training Loss: 0.4446\n",
      "Test Loss: 0.4908\n",
      "\n",
      "Model Performance Metrics:\n",
      "accuracy:\n",
      "0.8380192180588744\n",
      "\n",
      "precision:\n",
      "0.808637205237031\n",
      "\n",
      "recall:\n",
      "0.8380192180588744\n",
      "\n",
      "f1_score:\n",
      "0.812879137612259\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 7294    14  2873]\n",
      " [  731     9  1367]\n",
      " [ 1383     4 25663]]\n",
      "\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.74     10181\n",
      "           1       0.33      0.00      0.01      2107\n",
      "           2       0.86      0.95      0.90     27050\n",
      "\n",
      "    accuracy                           0.84     39338\n",
      "   macro avg       0.66      0.56      0.55     39338\n",
      "weighted avg       0.81      0.84      0.81     39338\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Modified LSTM Model\n",
   "id": "633fbb4d51c13651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:46:27.467506Z",
     "start_time": "2024-12-07T07:46:27.263812Z"
    }
   },
   "cell_type": "code",
   "source": "from imblearn.over_sampling import SMOTE",
   "id": "18e7b0a929dc485f",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:46:29.407360Z",
     "start_time": "2024-12-07T07:46:29.402832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss for handling class imbalance in classification tasks.\n",
    "    \n",
    "    Focal Loss modifies standard cross entropy by reducing the loss contribution from easy examples\n",
    "    and increasing the importance of hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma=2):\n",
    "        \"\"\"\n",
    "        :prarm alpha: A tensor of weights for each class to handle class imbalance\n",
    "        :param gamma: Focusing parameter that adjusts how much to down-weight easy examples (default: 2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # 类别权重\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculate the focal loss.\n",
    "        \n",
    "        :param inputs: Model predictions (logits)\n",
    "        :param targets: Ground truth labels\n",
    "        \n",
    "        :returns focal_loss: Computed focal loss value\n",
    "        \"\"\"\n",
    "        # Calculate standard cross entropy loss for each sample\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        \n",
    "        # Calculate probability of correct class (pt)\n",
    "        # pt closer to 1 means the model is more confident in its prediction\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Calculate focal loss:\n",
    "        # - For easy examples (pt close to 1), (1-pt)**gamma reduces their contribution\n",
    "        # - For hard examples (pt close to 0), (1-pt)**gamma keeps their contribution high\n",
    "        focal_loss = ((1-pt)**self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ],
   "id": "374385961c81971e",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:46:31.749668Z",
     "start_time": "2024-12-07T07:46:31.742354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## LSTMmodelNew\n",
    "class SentimentLSTM_new(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 dropout, \n",
    "                 app_feature_dim,\n",
    "                 bidirectional=True):  # Whether to use bidirectional LSTM\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional \n",
    "        )\n",
    "        \n",
    "        # Attention layer to focus on important parts of the sequence\n",
    "        self.attention = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 1)\n",
    "        \n",
    "        # Calculate LSTM output dimension based on directionality\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim + app_feature_dim, hidden_dim), # Combine features\n",
    "            nn.ReLU(), # Non-linear activation\n",
    "            nn.Dropout(dropout), # Regularization\n",
    "            nn.BatchNorm1d(hidden_dim), # Normalize activations\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), # Reduce dimension\n",
    "            nn.ReLU(), # Non-linear activation\n",
    "            nn.Dropout(dropout), # Additional regularization\n",
    "            nn.BatchNorm1d(hidden_dim // 2) # Final normalization\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "        # Layer normalization for LSTM output\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2 if bidirectional else hidden_dim)\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # Calculate attention weights for each time step\n",
    "        attn_weights = F.softmax(\n",
    "            self.attention(lstm_output).squeeze(-1), dim=1\n",
    "        )\n",
    "         # Apply attention weights to get context vector\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1), \n",
    "            lstm_output\n",
    "        ).squeeze(1)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, text, app_features):\n",
    "        # Apply word embeddings with dropout\n",
    "        embedded = F.dropout(self.embedding(text), 0.3, training=self.training)\n",
    "        \n",
    "        # Process sequence through LSTM\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output = self.attention_net(\n",
    "            lstm_output,\n",
    "            hidden\n",
    "        )\n",
    "        # Normalize LSTM output\n",
    "        normalized_output = self.layer_norm(attn_output)\n",
    "        \n",
    "        # Combine LSTM output with application features\n",
    "        combined = torch.cat([normalized_output, app_features], dim=1)\n",
    "        \n",
    "        # Process through feature fusion network\n",
    "        fused_features = self.feature_fusion(combined)\n",
    "        \n",
    "        return self.fc(fused_features)"
   ],
   "id": "c88eb5f21b300887",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:07:08.380772Z",
     "start_time": "2024-12-07T08:07:08.346120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentAnalyzer_new:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 content_col: str = 'cleaned_review', \n",
    "                 label_col: str = 'flag',\n",
    "                 app_col: str = 'app',\n",
    "                 test_size: float = 0.2,\n",
    "                 random_state: int = 42):\n",
    "        \n",
    "        # Store input DataFrame and device\n",
    "        self.df = df.copy()\n",
    "        self.device = device\n",
    "        \n",
    "        self.df['processed_text'] = self.df[content_col].apply(self.preprocess_text)\n",
    "        \n",
    "        # Build vocabulary and create word-to-index mapping\n",
    "        self.vocab = self.build_vocab(self.df['processed_text'])\n",
    "        self.word_to_idx = {word: i+1 for i, word in enumerate(self.vocab)}\n",
    "        self.word_to_idx['<PAD>'] = 0\n",
    "        self.word_to_idx['<UNK>'] = len(self.word_to_idx)\n",
    "        \n",
    "        self.encoded_texts = self.encode_texts(self.df['processed_text'])\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(label_encoder.fit_transform(self.df[label_col]))\n",
    "        \n",
    "        app_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.app_features = torch.tensor(\n",
    "            app_encoder.fit_transform(self.df[app_col].values.reshape(-1, 1)),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "       \n",
    "        (self.X_train, self.X_test, \n",
    "         self.app_train, self.app_test,\n",
    "         self.y_train, self.y_test) = train_test_split(\n",
    "            self.encoded_texts, \n",
    "            self.app_features,\n",
    "            self.labels, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=self.labels\n",
    "        )\n",
    "        \n",
    "        \n",
    "        batch_size = len(self.X_train)\n",
    "        seq_length = self.X_train.shape[1]  \n",
    "        embedding_dim = 1  \n",
    "        \n",
    "    \n",
    "        X_train_reshaped = self.X_train.reshape(batch_size, -1)\n",
    "        \n",
    "        \n",
    "        X_train_combined = torch.cat([\n",
    "            X_train_reshaped,\n",
    "            self.app_train\n",
    "        ], dim=1).numpy()\n",
    "        \n",
    "        \n",
    "        print(\"Applying SMOTE oversampling...\")\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_combined, self.y_train.numpy())\n",
    "        \n",
    "        \n",
    "        text_features_dim = seq_length * embedding_dim\n",
    "        \n",
    "        \n",
    "        X_text_resampled = X_resampled[:, :text_features_dim]\n",
    "        X_app_resampled = X_resampled[:, text_features_dim:]\n",
    "        \n",
    "        \n",
    "        self.X_train = torch.tensor(\n",
    "            X_text_resampled.reshape(-1, seq_length),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        self.app_train = torch.tensor(X_app_resampled, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y_resampled)\n",
    "        \n",
    "        print(f\"After SMOTE - Class distribution: {np.bincount(y_resampled)}\")\n",
    "        \n",
    "        \n",
    "        self.X_train = self.X_train.to(device)\n",
    "        self.X_test = self.X_test.to(device)\n",
    "        self.app_train = self.app_train.to(device)\n",
    "        self.app_test = self.app_test.to(device)\n",
    "        self.y_train = self.y_train.to(device)\n",
    "        self.y_test = self.y_test.to(device)\n",
    "        \n",
    "        self.train_dataset = ReviewDataset(self.X_train, self.app_train, self.y_train)\n",
    "        self.test_dataset = ReviewDataset(self.X_test, self.app_test, self.y_test)\n",
    "        \n",
    "        self.model_params = {\n",
    "            'vocab_size': len(self.word_to_idx),\n",
    "            'embedding_dim': 200,\n",
    "            'hidden_dim': 256,\n",
    "            'output_dim': len(np.unique(self.labels)),\n",
    "            'n_layers': 3,\n",
    "            'dropout': 0.3,\n",
    "            'app_feature_dim': self.app_features.shape[1],\n",
    "            'bidirectional': True\n",
    "        }\n",
    "        \n",
    "        self.model = SentimentLSTM_new(**self.model_params).to(device)\n",
    "        \n",
    "        # 计算类别权重并移到GPU\n",
    "        class_weights = self.calculate_class_weights().to(device)\n",
    "        self.criterion = FocalLoss(alpha=class_weights)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=2e-4,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=2e-4,\n",
    "            epochs=10,\n",
    "            steps_per_epoch=len(self.train_dataset) // 32 + 1,\n",
    "            pct_start=0.3\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return ''\n",
    "            \n",
    "        text = text.lower()\n",
    "        \n",
    "        def tokenize_with_emojis(text):\n",
    "            tokens = []\n",
    "            current_token = []\n",
    "            \n",
    "            for char in text:\n",
    "                if emoji.is_emoji(char):\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "                    tokens.append(char)\n",
    "                elif char.isalnum():\n",
    "                    current_token.append(char)\n",
    "                else:\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "            \n",
    "            if current_token:\n",
    "                tokens.append(''.join(current_token))\n",
    "            \n",
    "            return tokens\n",
    "        \n",
    "        tokens = tokenize_with_emojis(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words or emoji.is_emoji(token)]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def build_vocab(self, texts: pd.Series, max_vocab_size: int = 10000) -> List[str]:\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for token in text.split():\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "        \n",
    "        vocab = sorted(word_freq, key=word_freq.get, reverse=True)[:max_vocab_size]\n",
    "        return vocab\n",
    "    \n",
    "    def encode_texts(self, texts: pd.Series, max_length: int = 100) -> torch.Tensor:\n",
    "        encoded_texts = []\n",
    "        for text in texts:\n",
    "            indices = [\n",
    "                self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
    "                for token in text.split()\n",
    "            ]\n",
    "            \n",
    "            if len(indices) > max_length:\n",
    "                indices = indices[:max_length]\n",
    "            else:\n",
    "                indices = indices + [self.word_to_idx['<PAD>']] * (max_length - len(indices))\n",
    "            \n",
    "            encoded_texts.append(indices)\n",
    "        \n",
    "        return torch.tensor(encoded_texts)\n",
    "    \n",
    "    def calculate_class_weights(self):\n",
    "        class_counts = torch.bincount(self.y_train)\n",
    "        total = len(self.y_train)\n",
    "        weights = total / (len(class_counts) * class_counts)\n",
    "        return weights\n",
    "    \n",
    "    def train(self, epochs: int = 10, batch_size: int = 32):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            for texts, app_features, labels in pbar:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # 移除混合精度训练，直接计算\n",
    "                outputs = self.model(texts, app_features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # 正常的反向传播\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.scheduler.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                pbar.set_postfix({'train_loss': total_train_loss/len(train_loader)})\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            \n",
    "            # 评估阶段\n",
    "            self.model.eval()\n",
    "            total_test_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for texts, app_features, labels in test_loader:\n",
    "                    outputs = self.model(texts, app_features)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    total_test_loss += loss.item()\n",
    "                    \n",
    "            avg_test_loss = total_test_loss / len(test_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_test_loss < best_loss:\n",
    "                best_loss = avg_test_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    self.model.load_state_dict(torch.load('best_model.pt'))\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, app_features, labels in test_loader:\n",
    "                outputs = self.model(texts, app_features)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
    "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
    "            'f1_score': f1_score(all_labels, all_preds, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "            'classification_report': classification_report(all_labels, all_preds)\n",
    "        }\n",
    "        \n",
    "        return metrics"
   ],
   "id": "fd25834098831242",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:48:16.091349Z",
     "start_time": "2024-12-07T08:07:13.674707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use GPU to train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_seed(507)\n",
    "analyzer = SentimentAnalyzer_new(new_df)\n",
    "        \n",
    "# Train the model\n",
    "analyzer.train(epochs=30)\n",
    "        \n",
    "# Evaluate the model\n",
    "metrics = analyzer.evaluate()\n",
    "        \n",
    "# Print metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")"
   ],
   "id": "20af7a6b6f6291a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE oversampling...\n",
      "After SMOTE - Class distribution: [108196 108196 108196]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 10144/10144 [41:02<00:00,  4.12it/s, train_loss=0.509]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "Training Loss: 0.5086\n",
      "Test Loss: 0.3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 10144/10144 [39:30<00:00,  4.28it/s, train_loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30:\n",
      "Training Loss: 0.3816\n",
      "Test Loss: 0.3065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 10144/10144 [4:36:56<00:00,  1.64s/it, train_loss=0.351]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30:\n",
      "Training Loss: 0.3509\n",
      "Test Loss: 0.2930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 10144/10144 [53:46<00:00,  3.14it/s, train_loss=0.33]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30:\n",
      "Training Loss: 0.3304\n",
      "Test Loss: 0.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 10144/10144 [39:36<00:00,  4.27it/s, train_loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30:\n",
      "Training Loss: 0.3156\n",
      "Test Loss: 0.3337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 10144/10144 [40:42<00:00,  4.15it/s, train_loss=0.304]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30:\n",
      "Training Loss: 0.3035\n",
      "Test Loss: 0.2917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 10144/10144 [40:06<00:00,  4.22it/s, train_loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30:\n",
      "Training Loss: 0.2927\n",
      "Test Loss: 0.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 10144/10144 [41:04<00:00,  4.12it/s, train_loss=0.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30:\n",
      "Training Loss: 0.2844\n",
      "Test Loss: 0.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 10144/10144 [41:01<00:00,  4.12it/s, train_loss=0.278]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[94], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m analyzer \u001B[38;5;241m=\u001B[39m SentimentAnalyzer_new(new_df)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[43manalyzer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[1;32m     11\u001B[0m metrics \u001B[38;5;241m=\u001B[39m analyzer\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "Cell \u001B[0;32mIn[93], line 239\u001B[0m, in \u001B[0;36mSentimentAnalyzer_new.train\u001B[0;34m(self, epochs, batch_size)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m texts, app_features, labels \u001B[38;5;129;01min\u001B[39;00m test_loader:\n\u001B[0;32m--> 239\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n\u001B[1;32m    241\u001B[0m         total_test_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[81], line 55\u001B[0m, in \u001B[0;36mSentimentLSTM_new.forward\u001B[0;34m(self, text, app_features)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, app_features):\n\u001B[1;32m     53\u001B[0m     embedded \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(text), \u001B[38;5;241m0.3\u001B[39m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[0;32m---> 55\u001B[0m     lstm_output, (hidden, cell) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_net(\n\u001B[1;32m     58\u001B[0m         lstm_output,\n\u001B[1;32m     59\u001B[0m         hidden\n\u001B[1;32m     60\u001B[0m     )\n\u001B[1;32m     62\u001B[0m     normalized_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(attn_output)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1123\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m   1120\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1123\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1124\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1133\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1135\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[1;32m   1136\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1137\u001B[0m         batch_sizes,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[1;32m   1145\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 94
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
