{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sentiment Analysis for Reviews from 20 Apps",
   "id": "94eab9e1417c24e1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:34.678397Z",
     "start_time": "2024-12-08T07:25:34.363103Z"
    }
   },
   "source": [
    "from encodings import search_function\n",
    "import os\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:35.069601Z",
     "start_time": "2024-12-08T07:25:34.699760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read data\n",
    "data_path = \"data/all_combined.csv\"\n",
    "odf = pd.read_csv(data_path, encoding='utf-8', on_bad_lines=\"skip\")"
   ],
   "id": "febf1efc8d58ab30",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Clean Data and Establish new tag\n",
    "1. To classify the reviews into 3 groups: negative, neutral and positive, I use a new mapping rule to create a new field called `flag`\n",
    "2. Create a new flag to indicate whether the content contains emojis. It's unknown that whether the existence of emojis will affect the performance of the model\n",
    "3. Drop some non-English reviews\n",
    "4. Drop blank data"
   ],
   "id": "85dca54b6549b4fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:35.127901Z",
     "start_time": "2024-12-08T07:25:35.122601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create new field to re-classify the score\n",
    "score_to_flag = {1:-1, 2:-1, 3:0, 4:1, 5:1}\n",
    "odf['flag'] = odf['score'].map(score_to_flag)"
   ],
   "id": "e445f2cd029c63b6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:36.812693Z",
     "start_time": "2024-12-08T07:25:35.601951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to show if the content includes emoji\n",
    "def contains_emoji(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "odf[\"has_emoji\"] = odf[\"content\"].apply(contains_emoji)"
   ],
   "id": "519eb17363d7375a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:41.744402Z",
     "start_time": "2024-12-08T07:25:38.976625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter whose score is not in 1-5\n",
    "new_df = odf[odf[\"score\"].between(1, 5, inclusive=\"both\")]\n",
    "\n",
    "# strip out of non-ascii char or non-emoji part in the content\n",
    "def clean_review_content(text):\n",
    "    \"\"\"\n",
    "    Clean review content by:\n",
    "    1. Preserving emojis\n",
    "    2. Removing non-ASCII characters except emojis\n",
    "    3. Stripping extra whitespaces\n",
    "    \n",
    "    :param text (str): Input review text\n",
    "    :param str: Cleaned review text\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''\n",
    "    try:\n",
    "        # Extract emojis and save them\n",
    "        emojis = ''.join(c for c in text if c in emoji.EMOJI_DATA)\n",
    "        \n",
    "        # Remove non-ASCII characters, keeping emojis\n",
    "        cleaned_text = ''.join(c for c in text if (ord(c) < 128) or (c in emoji.EMOJI_DATA))\n",
    "        \n",
    "        # Remove multiple whitespaces and trim\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        \n",
    "        # Combine cleaned text with preserved emojis\n",
    "        return (cleaned_text + ' ' + emojis).strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "new_df[\"cleaned_review\"] = new_df[\"content\"].apply(clean_review_content)\n"
   ],
   "id": "30c5a7c33b238e54",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:42.741962Z",
     "start_time": "2024-12-08T07:25:42.675083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop NULL data\n",
    "new_df = new_df.dropna(subset=[\"cleaned_review\", \"score\", \"content\"])\n",
    "new_df = new_df[new_df[\"cleaned_review\"] != \"\"]\n",
    "print(f\"The original data size: {odf.shape}\")\n",
    "print(f\"The cleaned data size: {new_df.shape}\")"
   ],
   "id": "80826fc256cd9b0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data size: (200000, 6)\n",
      "The cleaned data size: (196688, 7)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T06:56:36.892080Z",
     "start_time": "2024-12-08T06:56:36.877028Z"
    }
   },
   "cell_type": "code",
   "source": "new_df.head(20)",
   "id": "6cb1c3b5d2f23822",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                reviewId  \\\n",
       "0   e2996bb1-cdf1-4f76-a4e5-88d47b3b8d5e   \n",
       "1   d32653e0-f81f-43d5-8d61-e0d5ce419eb0   \n",
       "2   6a4cd47e-44ee-4bfd-b1f7-f0c044e8419e   \n",
       "3   d8e578db-d679-4fd6-ab11-cef028134049   \n",
       "4   f6b1b7d5-5028-42c8-bb30-b57c6bf1a02b   \n",
       "5   3130b4cf-53b7-4823-8669-a44e52e3a3d5   \n",
       "6   9b870115-9d74-4d97-bcc3-37ae9dd0a1a2   \n",
       "7   0f45902f-ed8b-4a8a-a0d5-dda4e06d30b2   \n",
       "8   88e523fa-e2f4-4ad1-a40b-4315dcf56345   \n",
       "9   3aa36269-df4a-4eb3-919a-c92d45d10af5   \n",
       "10  1d7f9eec-726a-44fd-ae1d-f96456113d09   \n",
       "11  e562a0d4-6ede-4210-bf9b-8d6de7ce2fa6   \n",
       "13  456ac811-fff2-404d-bcb1-66df63e99966   \n",
       "14  3c34abfd-9f32-470e-b961-c91be564896d   \n",
       "15  9ab88748-7f9b-47fb-a53d-135ac917ba67   \n",
       "16  d7bc41fe-3d08-49f3-9e62-e348f570f052   \n",
       "17  f741ff16-19e2-4ae3-b1f4-921079a5e829   \n",
       "18  ed537838-524e-4b22-8760-54112723c391   \n",
       "19  bd801db6-96ff-4145-a67c-7ea49f66b848   \n",
       "20  29977c65-264e-4a10-8130-8aa97c7d5791   \n",
       "\n",
       "                                              content  score       app  flag  \\\n",
       "0                                                 Oop      5  Facebook     1   \n",
       "1                              Facebook is a nice app      5  Facebook     1   \n",
       "2                                                best      5  Facebook     1   \n",
       "3                                Open Facebook update      5  Facebook     1   \n",
       "4   Facebook bhoot aacha chize hai aap sabhi log b...      5  Facebook     1   \n",
       "5                                            Nice 💯👍🫦      5  Facebook     1   \n",
       "6            Too many annoying useless notifications.      1  Facebook    -1   \n",
       "7                      Soferrr gandaaa Siya sistaa!!!      5  Facebook     1   \n",
       "8                                            Best 👌 👍      4  Facebook     1   \n",
       "9                                                nice      5  Facebook     1   \n",
       "10                                           So crazy      1  Facebook    -1   \n",
       "11                                              🔥🔥🔥🥰🥰      5  Facebook     1   \n",
       "13                                               Nice      5  Facebook     1   \n",
       "14                                      hii fb family      5  Facebook     1   \n",
       "15                                           Good app      5  Facebook     1   \n",
       "16                                                  😞      5  Facebook     1   \n",
       "17                                        very good 👍      5  Facebook     1   \n",
       "18                                        very good 👍      5  Facebook     1   \n",
       "19                                               good      5  Facebook     1   \n",
       "20                                          Great day      5  Facebook     1   \n",
       "\n",
       "    has_emoji                                     cleaned_review  \n",
       "0       False                                                Oop  \n",
       "1       False                             Facebook is a nice app  \n",
       "2       False                                               best  \n",
       "3       False                               Open Facebook update  \n",
       "4        True  Facebook bhoot aacha chize hai aap sabhi log b...  \n",
       "5        True                                       Nice 💯👍🫦 💯👍🫦  \n",
       "6       False           Too many annoying useless notifications.  \n",
       "7       False                     Soferrr gandaaa Siya sistaa!!!  \n",
       "8        True                                        Best 👌 👍 👌👍  \n",
       "9       False                                               nice  \n",
       "10      False                                           So crazy  \n",
       "11       True                                        🔥🔥🔥🥰🥰 🔥🔥🔥🥰🥰  \n",
       "13      False                                               Nice  \n",
       "14      False                                      hii fb family  \n",
       "15      False                                           Good app  \n",
       "16       True                                                😞 😞  \n",
       "17       True                                      very good 👍 👍  \n",
       "18       True                                      very good 👍 👍  \n",
       "19      False                                               good  \n",
       "20      False                                          Great day  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>app</th>\n",
       "      <th>flag</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e2996bb1-cdf1-4f76-a4e5-88d47b3b8d5e</td>\n",
       "      <td>Oop</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Oop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d32653e0-f81f-43d5-8d61-e0d5ce419eb0</td>\n",
       "      <td>Facebook is a nice app</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Facebook is a nice app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6a4cd47e-44ee-4bfd-b1f7-f0c044e8419e</td>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d8e578db-d679-4fd6-ab11-cef028134049</td>\n",
       "      <td>Open Facebook update</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Open Facebook update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f6b1b7d5-5028-42c8-bb30-b57c6bf1a02b</td>\n",
       "      <td>Facebook bhoot aacha chize hai aap sabhi log b...</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook bhoot aacha chize hai aap sabhi log b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3130b4cf-53b7-4823-8669-a44e52e3a3d5</td>\n",
       "      <td>Nice 💯👍🫦</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Nice 💯👍🫦 💯👍🫦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9b870115-9d74-4d97-bcc3-37ae9dd0a1a2</td>\n",
       "      <td>Too many annoying useless notifications.</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Too many annoying useless notifications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0f45902f-ed8b-4a8a-a0d5-dda4e06d30b2</td>\n",
       "      <td>Soferrr gandaaa Siya sistaa!!!</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Soferrr gandaaa Siya sistaa!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88e523fa-e2f4-4ad1-a40b-4315dcf56345</td>\n",
       "      <td>Best 👌 👍</td>\n",
       "      <td>4</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Best 👌 👍 👌👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3aa36269-df4a-4eb3-919a-c92d45d10af5</td>\n",
       "      <td>nice</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1d7f9eec-726a-44fd-ae1d-f96456113d09</td>\n",
       "      <td>So crazy</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>So crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e562a0d4-6ede-4210-bf9b-8d6de7ce2fa6</td>\n",
       "      <td>🔥🔥🔥🥰🥰</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>🔥🔥🔥🥰🥰 🔥🔥🔥🥰🥰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>456ac811-fff2-404d-bcb1-66df63e99966</td>\n",
       "      <td>Nice</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3c34abfd-9f32-470e-b961-c91be564896d</td>\n",
       "      <td>hii fb family</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>hii fb family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9ab88748-7f9b-47fb-a53d-135ac917ba67</td>\n",
       "      <td>Good app</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Good app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d7bc41fe-3d08-49f3-9e62-e348f570f052</td>\n",
       "      <td>😞</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>😞 😞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>f741ff16-19e2-4ae3-b1f4-921079a5e829</td>\n",
       "      <td>very good 👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>very good 👍 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ed537838-524e-4b22-8760-54112723c391</td>\n",
       "      <td>very good 👍</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>very good 👍 👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bd801db6-96ff-4145-a67c-7ea49f66b848</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29977c65-264e-4a10-8130-8aa97c7d5791</td>\n",
       "      <td>Great day</td>\n",
       "      <td>5</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Great day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Modelling",
   "id": "853e148aafbea446"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:51.010977Z",
     "start_time": "2024-12-08T07:25:47.270623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import necessary packages\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "# natural language tool kits\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "465ee52d859d2522",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:51.197913Z",
     "start_time": "2024-12-08T07:25:51.017154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# download necessary NLTK resources\n",
    "# stopwords: words such as \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "## set random seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "id": "6dcfa96ee085f309",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:25:53.361830Z",
     "start_time": "2024-12-08T07:25:53.358524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the dataset with reviews, features and labels.\n",
    "    \n",
    "    \n",
    "    :param texts: List/array of review texts\n",
    "    :param app_features: List/array of feature vectors for each app\n",
    "    :param lables: List/array of corresponding labels/ratings\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, app_features, lables):\n",
    "        self.texts = texts\n",
    "        self.app_features = app_features\n",
    "        self.lables = lables\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the total number of samples in the dataset.\n",
    "    Required by PyTorch Dataset class.\n",
    "    \n",
    "    :returns int: Number of reviews/samples in the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetch a single sample from the dataset at the specified index.\n",
    "    Required by PyTorch Dataset class.\n",
    "    \n",
    "    :param idx: Index of the sample to retrieve\n",
    "    :returns tuple: (review_text, app_features, label) for the specified index\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.app_features[idx], self.lables[idx]"
   ],
   "id": "6158251f8976af",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T06:46:23.940706Z",
     "start_time": "2024-12-08T06:46:23.934276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, hidden_dim, n_layers, dropout, app_feature_dim):\n",
    "        \"\"\"\n",
    "        Initialize LSTM Model for Sentiment Classification with App Features\n",
    "        \n",
    "        :param vocab_size (int): Total number of unique words in vocabulary\n",
    "        :param embedding_dim (int): Size of word embedding vectors\n",
    "        :param output_dim (int): Number of output classes (e.g., 2 for binary sentiment)\n",
    "        :param hidden_dim (int): Number of features in LSTM hidden state\n",
    "        :param n_layers (int): Number of stacked LSTM layers\n",
    "        :param dropout (float): Dropout rate for regularization\n",
    "        :param app_feature_dim (int): Dimension of additional app-specific features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Create embedding layer to convert word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer processes sequence of word embeddings\n",
    "        # batch_first=True means input shape is (batch_size, sequence_length, features)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=dropout if n_layers > 1 else 0,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # First fully connected layer combines LSTM output with app features\n",
    "        self.fc1 = nn.Linear(hidden_dim + app_feature_dim, 128)\n",
    "        \n",
    "        # Output layer produces final classification\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        \n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ReLU activation function adds non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, text, app_features):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        \n",
    "        :param text (torch.Tensor): Input tensor of word indices, shape (batch_size, sequence_length)\n",
    "        :param app_features (torch.Tensor): Additional app-specific features, shape (batch_size, app_feature_dim)\n",
    "        \n",
    "        \n",
    "        :returns torch.Tensor: Model predictions, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings and apply dropout\n",
    "        embedded = self.dropout(self.embedding(text)) # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Process sequence through LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden state from last LSTM layer\n",
    "        hidden = self.dropout(hidden[-1, :, :])\n",
    "        # Concatenate hidden state with app features\n",
    "        combined = torch.cat([hidden, app_features], dim=1) # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Pass through fully connected layers with dropout and ReLU\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Return final predictions\n",
    "        return self.fc2(x)"
   ],
   "id": "43f8bd9b04083967",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T05:34:58.131707Z",
     "start_time": "2024-12-07T05:34:58.115690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentAnalyzer:\n",
    "    \n",
    "    \"\"\"\n",
    "    A comprehensive sentiment analysis pipeline that processes app reviews\n",
    "    Combines text processing, LSTM modeling, and app-specific features\n",
    "    \n",
    "    Key Features:\n",
    "    - Text preprocessing with emoji preservation\n",
    "    - Custom vocabulary building\n",
    "    - LSTM-based sentiment classification\n",
    "    - Integration of app-specific features\n",
    "    - Built-in training and evaluation pipeline\n",
    "    \n",
    "    :param df: DataFrame containing review data\n",
    "    :param content_col: Name of column containing review text\n",
    "    :param label_col: Name of column containing sentiment labels\n",
    "    :param app_col: Name of column containing app identifiers\n",
    "    :param test_size: Proportion of data to use for testing (0-1)\n",
    "    :param random_state: Random seed for reproducible results\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, content_col: str = \"cleaned_review\",\n",
    "                 label_col: str = \"flag\", app_col: str = \"app\", \n",
    "                 test_size: float = 0.2, random_state: int = 42):\n",
    "        \n",
    "        # Create a copy of input data to avoid modifications to original\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # Preprocess all review texts\n",
    "        self.df['processed_text'] = self.df[content_col].apply(self.preprocess_text)\n",
    "        \n",
    "        # Build vocabulary from processed texts\n",
    "        self.vocab = self.build_vocab(self.df['processed_text'])\n",
    "        \n",
    "        # Create word-to-index mapping with special tokens\n",
    "        self.word_to_idx = {word: i + 1 for i, word in enumerate(self.vocab)}\n",
    "        self.word_to_idx['<PAD>'] = 0  \n",
    "        self.word_to_idx['<UNK>'] = len(self.word_to_idx)\n",
    "        \n",
    "        # Convert texts to numerical sequences\n",
    "        self.encoded_texts = self.encode_texts(self.df['processed_text'])\n",
    "        \n",
    "        # Encode sentiment labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(label_encoder.fit_transform(self.df[label_col]))\n",
    "        \n",
    "        # One-hot encode app features\n",
    "        app_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.app_features = torch.tensor(\n",
    "            app_encoder.fit_transform(self.df[app_col].values.reshape(-1, 1)),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        self.X_train, self.X_test, self.app_train, self.app_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.encoded_texts,\n",
    "            self.app_features,\n",
    "            self.labels,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=self.labels\n",
    "        )\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        self.train_dataset = ReviewDataset(self.X_train, self.app_train, self.y_train)\n",
    "        self.test_dataset = ReviewDataset(self.X_test, self.app_test, self.y_test)\n",
    "        \n",
    "        # Define model hyperparameters\n",
    "        self.model_params = {\n",
    "            'vocab_size': len(self.word_to_idx),\n",
    "            'embedding_dim': 100,\n",
    "            'hidden_dim': 128,\n",
    "            'output_dim': len(np.unique(self.labels)),\n",
    "            'n_layers': 2,\n",
    "            'dropout': 0.5,\n",
    "            'app_feature_dim': self.app_features.shape[1]\n",
    "        }\n",
    "        \n",
    "       # Initialize model, loss function, and optimizer\n",
    "        self.model = SentimentLSTM(**self.model_params)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "    \n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text while preserving emojis\n",
    "        \n",
    "        Steps:\n",
    "        1. Convert to lowercase\n",
    "        2. Tokenize while keeping emojis intact\n",
    "        3. Remove stopwords (except emojis)\n",
    "        4. Join tokens back into text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        def tokenize_with_emojis(text):\n",
    "            \"\"\"\n",
    "            Custom tokenizer that preserves emojis as distinct tokens\n",
    "            Handles the text character by character to properly separate\n",
    "            emojis from regular words\n",
    "            \"\"\"\n",
    "            tokens = []\n",
    "            current_token = []\n",
    "            \n",
    "            for char in text:\n",
    "                if emoji.is_emoji(char):\n",
    "                    # If there's a current token, save it\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "                    # Add emoji as a separate token\n",
    "                    tokens.append(char)\n",
    "                elif char.isalnum():\n",
    "                    current_token.append(char)\n",
    "                else:\n",
    "                    # If there's a current token, save it\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "            \n",
    "            # Handle any remaining token\n",
    "            if current_token:\n",
    "                tokens.append(''.join(current_token))\n",
    "            \n",
    "            return tokens\n",
    "        \n",
    "        # Tokenize and remove stopwords (preserve emojis)\n",
    "        tokens = tokenize_with_emojis(text)  \n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [token for token in tokens if token not in stop_words or emoji.is_emoji(token)]\n",
    "        return ' '.join(tokens) \n",
    "    \n",
    "    def build_vocab(self, texts: pd.Series, max_vocab_size: int = 10000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from texts, including emojis\n",
    "        \n",
    "        \n",
    "        :param texts (pd.Series): Series of processed texts\n",
    "        :param max_vocab_size (int): Maximum vocabulary size\n",
    "        \n",
    "        \n",
    "        :returns List[str]: Vocabulary words and emojis\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for token in text.split():\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "        \n",
    "        # Sort by frequency and limit vocabulary\n",
    "        vocab = sorted(word_freq, key=word_freq.get, reverse=True)[:max_vocab_size]\n",
    "        return vocab\n",
    "    \n",
    "    def encode_texts(self, texts: pd.Series, max_length: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode texts to tensor, handling emojis\n",
    "        \n",
    "        :param texts (pd.Series): Series of processed texts\n",
    "        :param max_length (int): Maximum sequence length\n",
    "        \n",
    "        :returns torch.Tensor: Encoded texts\n",
    "        \"\"\"\n",
    "        encoded_texts = []\n",
    "        for text in texts:\n",
    "            # Convert tokens to indices\n",
    "            indices = [\n",
    "                self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
    "                for token in text.split()\n",
    "            ]\n",
    "            \n",
    "            # Pad or truncate to fixed length\n",
    "            if len(indices) > max_length:\n",
    "                indices = indices[:max_length]\n",
    "            else:\n",
    "                indices = indices + [self.word_to_idx['<PAD>']] * (max_length - len(indices))\n",
    "            \n",
    "            encoded_texts.append(indices)\n",
    "        \n",
    "        return torch.tensor(encoded_texts)\n",
    "    \n",
    "    def train(self, epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Train the LSTM model\n",
    "        \n",
    "        \n",
    "        :param epochs (int): Number of training epochs\n",
    "        :param batch_size (int): Batch size for training\n",
    "        \"\"\"\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_total_loss = 0\n",
    "            \n",
    "            for texts, app_features, labels in train_loader:\n",
    "                # Clear gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(texts, app_features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_total_loss += loss.item()\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            avg_train_loss = train_total_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            test_total_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for texts, app_features, labels in test_loader:\n",
    "                    outputs = self.model(texts, app_features)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    test_total_loss += loss.item()\n",
    "            \n",
    "            # Calculate average test loss\n",
    "            avg_test_loss = test_total_loss / len(test_loader)\n",
    "            \n",
    "            # Print both losses\n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Test Loss: {avg_test_loss:.4f}\\n')\n",
    "            \n",
    "    def evaluate(self) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "        \n",
    "       \n",
    "        :returns dict: Performance metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, app_features, labels in test_loader:\n",
    "                outputs = self.model(texts, app_features)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
    "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
    "            'f1_score': f1_score(all_labels, all_preds, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "            'classification_report': classification_report(all_labels, all_preds)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "            "
   ],
   "id": "b38b37b741b5820b",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T06:06:54.390961Z",
     "start_time": "2024-12-07T05:35:07.419705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "        set_seed(507)\n",
    "        # Initialize Sentiment Analyzer\n",
    "        analyzer = SentimentAnalyzer(new_df)\n",
    "        \n",
    "        # Train the model\n",
    "        analyzer.train(epochs=10)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = analyzer.evaluate()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "1d1e4596d77eab35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 0.7297\n",
      "Test Loss: 0.7190\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.7216\n",
      "Test Loss: 0.7185\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.7208\n",
      "Test Loss: 0.7184\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.7201\n",
      "Test Loss: 0.7178\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.7197\n",
      "Test Loss: 0.7183\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.6469\n",
      "Test Loss: 0.5410\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.5349\n",
      "Test Loss: 0.5071\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.5115\n",
      "Test Loss: 0.4948\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.4992\n",
      "Test Loss: 0.4892\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.4900\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Model Performance Metrics:\n",
      "accuracy:\n",
      "0.8352483603640246\n",
      "\n",
      "precision:\n",
      "0.7878294310216243\n",
      "\n",
      "recall:\n",
      "0.8352483603640246\n",
      "\n",
      "f1_score:\n",
      "0.8099420265903265\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 7314     0  2867]\n",
      " [  752     0  1355]\n",
      " [ 1507     0 25543]]\n",
      "\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74     10181\n",
      "           1       0.00      0.00      0.00      2107\n",
      "           2       0.86      0.94      0.90     27050\n",
      "\n",
      "    accuracy                           0.84     39338\n",
      "   macro avg       0.54      0.55      0.55     39338\n",
      "weighted avg       0.79      0.84      0.81     39338\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/stats507/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T07:44:21.587198Z",
     "start_time": "2024-12-07T06:09:39.795368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# increase epochs\n",
    "def main():\n",
    "        set_seed(507)\n",
    "        # Initialize Sentiment Analyzer\n",
    "        analyzer = SentimentAnalyzer(new_df)\n",
    "        \n",
    "        # Train the model\n",
    "        analyzer.train(epochs=30)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = analyzer.evaluate()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "ed3ff38e4e7c6c05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "Training Loss: 0.7297\n",
      "Test Loss: 0.7190\n",
      "\n",
      "Epoch 2/30:\n",
      "Training Loss: 0.7216\n",
      "Test Loss: 0.7185\n",
      "\n",
      "Epoch 3/30:\n",
      "Training Loss: 0.7208\n",
      "Test Loss: 0.7184\n",
      "\n",
      "Epoch 4/30:\n",
      "Training Loss: 0.7201\n",
      "Test Loss: 0.7178\n",
      "\n",
      "Epoch 5/30:\n",
      "Training Loss: 0.7197\n",
      "Test Loss: 0.7183\n",
      "\n",
      "Epoch 6/30:\n",
      "Training Loss: 0.6469\n",
      "Test Loss: 0.5410\n",
      "\n",
      "Epoch 7/30:\n",
      "Training Loss: 0.5349\n",
      "Test Loss: 0.5071\n",
      "\n",
      "Epoch 8/30:\n",
      "Training Loss: 0.5115\n",
      "Test Loss: 0.4948\n",
      "\n",
      "Epoch 9/30:\n",
      "Training Loss: 0.4992\n",
      "Test Loss: 0.4892\n",
      "\n",
      "Epoch 10/30:\n",
      "Training Loss: 0.4900\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Epoch 11/30:\n",
      "Training Loss: 0.4838\n",
      "Test Loss: 0.4893\n",
      "\n",
      "Epoch 12/30:\n",
      "Training Loss: 0.4800\n",
      "Test Loss: 0.4833\n",
      "\n",
      "Epoch 13/30:\n",
      "Training Loss: 0.4758\n",
      "Test Loss: 0.4875\n",
      "\n",
      "Epoch 14/30:\n",
      "Training Loss: 0.4713\n",
      "Test Loss: 0.4840\n",
      "\n",
      "Epoch 15/30:\n",
      "Training Loss: 0.4689\n",
      "Test Loss: 0.4911\n",
      "\n",
      "Epoch 16/30:\n",
      "Training Loss: 0.4666\n",
      "Test Loss: 0.4861\n",
      "\n",
      "Epoch 17/30:\n",
      "Training Loss: 0.4641\n",
      "Test Loss: 0.4848\n",
      "\n",
      "Epoch 18/30:\n",
      "Training Loss: 0.4623\n",
      "Test Loss: 0.4887\n",
      "\n",
      "Epoch 19/30:\n",
      "Training Loss: 0.4599\n",
      "Test Loss: 0.4826\n",
      "\n",
      "Epoch 20/30:\n",
      "Training Loss: 0.4588\n",
      "Test Loss: 0.4823\n",
      "\n",
      "Epoch 21/30:\n",
      "Training Loss: 0.4568\n",
      "Test Loss: 0.5100\n",
      "\n",
      "Epoch 22/30:\n",
      "Training Loss: 0.4548\n",
      "Test Loss: 0.4867\n",
      "\n",
      "Epoch 23/30:\n",
      "Training Loss: 0.4522\n",
      "Test Loss: 0.4895\n",
      "\n",
      "Epoch 24/30:\n",
      "Training Loss: 0.4529\n",
      "Test Loss: 0.4857\n",
      "\n",
      "Epoch 25/30:\n",
      "Training Loss: 0.4513\n",
      "Test Loss: 0.5014\n",
      "\n",
      "Epoch 26/30:\n",
      "Training Loss: 0.4502\n",
      "Test Loss: 0.4976\n",
      "\n",
      "Epoch 27/30:\n",
      "Training Loss: 0.4485\n",
      "Test Loss: 0.5266\n",
      "\n",
      "Epoch 28/30:\n",
      "Training Loss: 0.4474\n",
      "Test Loss: 0.4851\n",
      "\n",
      "Epoch 29/30:\n",
      "Training Loss: 0.4465\n",
      "Test Loss: 0.4918\n",
      "\n",
      "Epoch 30/30:\n",
      "Training Loss: 0.4446\n",
      "Test Loss: 0.4908\n",
      "\n",
      "Model Performance Metrics:\n",
      "accuracy:\n",
      "0.8380192180588744\n",
      "\n",
      "precision:\n",
      "0.808637205237031\n",
      "\n",
      "recall:\n",
      "0.8380192180588744\n",
      "\n",
      "f1_score:\n",
      "0.812879137612259\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 7294    14  2873]\n",
      " [  731     9  1367]\n",
      " [ 1383     4 25663]]\n",
      "\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.74     10181\n",
      "           1       0.33      0.00      0.01      2107\n",
      "           2       0.86      0.95      0.90     27050\n",
      "\n",
      "    accuracy                           0.84     39338\n",
      "   macro avg       0.66      0.56      0.55     39338\n",
      "weighted avg       0.81      0.84      0.81     39338\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Modified LSTM Model\n",
   "id": "633fbb4d51c13651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:26:05.441638Z",
     "start_time": "2024-12-08T07:26:04.531143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "id": "18e7b0a929dc485f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:26:06.484125Z",
     "start_time": "2024-12-08T07:26:06.479184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_gpu_status():\n",
    "   if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        print(\"MPS (Apple Silicon GPU) is available\")\n",
    "        print(\"Using Apple Metal Performance Shaders (MPS) for GPU acceleration\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "   else:\n",
    "        print(\"MPS device not found, using CPU instead\")"
   ],
   "id": "c7c2b0df02afe86f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:26:07.850443Z",
     "start_time": "2024-12-08T07:26:07.846362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss for handling class imbalance in classification tasks.\n",
    "    \n",
    "    Focal Loss modifies standard cross entropy by reducing the loss contribution from easy examples\n",
    "    and increasing the importance of hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma=2):\n",
    "        \"\"\"\n",
    "        :prarm alpha: A tensor of weights for each class to handle class imbalance\n",
    "        :param gamma: Focusing parameter that adjusts how much to down-weight easy examples (default: 2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.register_buffer('alpha', alpha)  # 类别权重\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculate the focal loss.\n",
    "        \n",
    "        :param inputs: Model predictions (logits)\n",
    "        :param targets: Ground truth labels\n",
    "        \n",
    "        :returns focal_loss: Computed focal loss value\n",
    "        \"\"\"\n",
    "        # Calculate standard cross entropy loss for each sample\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        \n",
    "        # Calculate probability of correct class (pt)\n",
    "        # pt closer to 1 means the model is more confident in its prediction\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Calculate focal loss:\n",
    "        # - For easy examples (pt close to 1), (1-pt)**gamma reduces their contribution\n",
    "        # - For hard examples (pt close to 0), (1-pt)**gamma keeps their contribution high\n",
    "        focal_loss = ((1-pt)**self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ],
   "id": "374385961c81971e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:26:10.626636Z",
     "start_time": "2024-12-08T07:26:10.619788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## LSTMmodelNew\n",
    "class SentimentLSTM_new(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 dropout, \n",
    "                 app_feature_dim,\n",
    "                 bidirectional=True):  # Whether to use bidirectional LSTM\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=n_layers, \n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional \n",
    "        )\n",
    "        \n",
    "        # Attention layer to focus on important parts of the sequence\n",
    "        self.attention = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 1)\n",
    "        \n",
    "        # Calculate LSTM output dimension based on directionality\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim + app_feature_dim, hidden_dim), # Combine features\n",
    "            nn.ReLU(), # Non-linear activation\n",
    "            nn.Dropout(dropout), # Regularization\n",
    "            nn.BatchNorm1d(hidden_dim), # Normalize activations\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), # Reduce dimension\n",
    "            nn.ReLU(), # Non-linear activation\n",
    "            nn.Dropout(dropout), # Additional regularization\n",
    "            nn.BatchNorm1d(hidden_dim // 2) # Final normalization\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "        # Layer normalization for LSTM output\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2 if bidirectional else hidden_dim)\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # Calculate attention weights for each time step\n",
    "        attn_weights = F.softmax(\n",
    "            self.attention(lstm_output).squeeze(-1), dim=1\n",
    "        )\n",
    "         # Apply attention weights to get context vector\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1), \n",
    "            lstm_output\n",
    "        ).squeeze(1)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, text, app_features):\n",
    "        # Apply word embeddings with dropout\n",
    "        embedded = F.dropout(self.embedding(text), 0.3, training=self.training)\n",
    "        \n",
    "        # Process sequence through LSTM\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_output = self.attention_net(\n",
    "            lstm_output,\n",
    "            hidden\n",
    "        )\n",
    "        # Normalize LSTM output\n",
    "        normalized_output = self.layer_norm(attn_output)\n",
    "        \n",
    "        # Combine LSTM output with application features\n",
    "        combined = torch.cat([normalized_output, app_features], dim=1)\n",
    "        \n",
    "        # Process through feature fusion network\n",
    "        fused_features = self.feature_fusion(combined)\n",
    "        \n",
    "        return self.fc(fused_features)"
   ],
   "id": "c88eb5f21b300887",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:45:12.911804Z",
     "start_time": "2024-12-08T07:45:12.878222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentAnalyzer_new:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 content_col: str = 'cleaned_review', \n",
    "                 label_col: str = 'flag',\n",
    "                 app_col: str = 'app',\n",
    "                 test_size: float = 0.2,\n",
    "                 random_state: int = 42):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the sentiment analyzer with data preprocessing and model setup.\n",
    "        \n",
    "        \n",
    "        :param df: Input DataFrame containing review text and labels\n",
    "        :param content_col: Column name for review text\n",
    "        :param label_col: Column name for sentiment labels\n",
    "        :param app_col: Column name for app/product identifiers\n",
    "        :param test_size: Proportion of data to use for testing\n",
    "        :param random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        # Set up GPU/CPU device for computation\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        check_gpu_status()\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Create a copy of input data and preprocess text\n",
    "        self.df = df.copy()\n",
    "        self.df['processed_text'] = self.df[content_col].apply(self.preprocess_text)\n",
    "        \n",
    "        # Create vocabulary and word indexing system\n",
    "        self.vocab = self.build_vocab(self.df['processed_text'])\n",
    "        self.word_to_idx = {word: i+1 for i, word in enumerate(self.vocab)}\n",
    "        self.word_to_idx['<PAD>'] = 0\n",
    "        self.word_to_idx['<UNK>'] = len(self.word_to_idx)\n",
    "        \n",
    "        # Convert text to numerical sequences\n",
    "        self.encoded_texts = self.encode_texts(self.df['processed_text']).to(self.device)\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(\n",
    "            label_encoder.fit_transform(self.df[label_col]),\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # One-hot encode app/product features\n",
    "        app_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.app_features = torch.tensor(\n",
    "            app_encoder.fit_transform(self.df[app_col].values.reshape(-1, 1)),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # split the dataset into training set and test set on cpu\n",
    "        (self.X_train, self.X_test, \n",
    "         self.app_train, self.app_test,\n",
    "         self.y_train, self.y_test) = train_test_split(\n",
    "            self.encoded_texts.cpu().numpy(),\n",
    "            self.app_features.cpu().numpy(),\n",
    "            self.labels.cpu().numpy(), \n",
    "            test_size=test_size, \n",
    "            random_state=random_state,\n",
    "            stratify=self.labels.cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        # Prepare data for SMOTE oversampling\n",
    "        batch_size = len(self.X_train)\n",
    "        seq_length = self.X_train.shape[1]\n",
    "        embedding_dim = 1\n",
    "        \n",
    "        X_train_reshaped = np.reshape(self.X_train, (batch_size, -1))\n",
    "        X_train_combined = np.concatenate([X_train_reshaped, self.app_train], axis=1)\n",
    "        \n",
    "        print(\"Applying SMOTE oversampling...\")\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_combined, self.y_train)\n",
    "        \n",
    "        # Separate text and app features after SMOTE\n",
    "        text_features_dim = seq_length * embedding_dim\n",
    "        X_text_resampled = X_resampled[:, :text_features_dim]\n",
    "        X_app_resampled = X_resampled[:, text_features_dim:]\n",
    "        \n",
    "        # move processed data to the device\n",
    "        self.X_train = torch.tensor(\n",
    "            X_text_resampled.reshape(-1, seq_length),\n",
    "            dtype=torch.long,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.X_test = torch.tensor(self.X_test, dtype=torch.long, device=self.device)\n",
    "        self.app_train = torch.tensor(X_app_resampled, dtype=torch.float32, device=self.device)\n",
    "        self.app_test = torch.tensor(self.app_test, dtype=torch.float32, device=self.device)\n",
    "        self.y_train = torch.tensor(y_resampled, device=self.device)\n",
    "        self.y_test = torch.tensor(self.y_test, device=self.device)\n",
    "        \n",
    "        print(f\"After SMOTE - Class distribution: {np.bincount(y_resampled)}\")\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        self.train_dataset = ReviewDataset(self.X_train, self.app_train, self.y_train)\n",
    "        self.test_dataset = ReviewDataset(self.X_test, self.app_test, self.y_test)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        # initialize the model\n",
    "        self.model_params = {\n",
    "            'vocab_size': len(self.word_to_idx),\n",
    "            'embedding_dim': 200,\n",
    "            'hidden_dim': 256,\n",
    "            'output_dim': len(np.unique(self.labels.cpu().numpy())),\n",
    "            'n_layers': 3,\n",
    "            'dropout': 0.3,\n",
    "            'app_feature_dim': self.app_features.shape[1],\n",
    "            'bidirectional': True\n",
    "        }\n",
    "        \n",
    "        self.model = SentimentLSTM_new(**self.model_params).to(self.device)\n",
    "        \n",
    "        # set up loss function and optimizer\n",
    "        class_weights = self.calculate_class_weights()\n",
    "        self.criterion = FocalLoss(alpha=class_weights)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-4,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        #     self.optimizer,\n",
    "        #     max_lr=2e-4,\n",
    "        #     epochs=10,\n",
    "        #     steps_per_epoch=len(self.train_dataset) // 32 + 1,\n",
    "        #     pct_start=0.3\n",
    "        # )\n",
    "        \n",
    "        self.scheduler = None\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text by converting to lowercase, tokenizing, and removing stop words.\n",
    "        Preserves emojis as separate tokens.\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return ''\n",
    "            \n",
    "        text = text.lower()\n",
    "        \n",
    "        def tokenize_with_emojis(text):\n",
    "            \"\"\"Helper function to tokenize text while preserving emojis\"\"\"\n",
    "            tokens = []\n",
    "            current_token = []\n",
    "            \n",
    "            for char in text:\n",
    "                if emoji.is_emoji(char):\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "                    tokens.append(char)\n",
    "                elif char.isalnum():\n",
    "                    current_token.append(char)\n",
    "                else:\n",
    "                    if current_token:\n",
    "                        tokens.append(''.join(current_token))\n",
    "                        current_token = []\n",
    "            \n",
    "            if current_token:\n",
    "                tokens.append(''.join(current_token))\n",
    "            \n",
    "            return tokens\n",
    "        \n",
    "        tokens = tokenize_with_emojis(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words or emoji.is_emoji(token)]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def build_vocab(self, texts: pd.Series, max_vocab_size: int = 10000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from text data, limiting to most frequent words.\n",
    "        Returns list of words sorted by frequency.\n",
    "        \"\"\"\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for token in text.split():\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "        \n",
    "        vocab = sorted(word_freq, key=word_freq.get, reverse=True)[:max_vocab_size]\n",
    "        return vocab\n",
    "    \n",
    "    def encode_texts(self, texts: pd.Series, max_length: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert text to numerical sequences using word indices.\n",
    "        Pads or truncates sequences to specified length.\n",
    "        \"\"\"\n",
    "        encoded_texts = []\n",
    "        for text in texts:\n",
    "            indices = [\n",
    "                self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
    "                for token in text.split()\n",
    "            ]\n",
    "            \n",
    "            if len(indices) > max_length:\n",
    "                indices = indices[:max_length]\n",
    "            else:\n",
    "                indices = indices + [self.word_to_idx['<PAD>']] * (max_length - len(indices))\n",
    "            \n",
    "            encoded_texts.append(indices)\n",
    "        \n",
    "        return torch.tensor(encoded_texts)\n",
    "    \n",
    "    def calculate_class_weights(self):\n",
    "        \"\"\"Calculate class weights to handle class imbalance\"\"\"\n",
    "        class_counts = torch.bincount(self.y_train)\n",
    "        total = len(self.y_train)\n",
    "        weights = total / (len(class_counts) * class_counts)\n",
    "        return weights.to(self.device)\n",
    "    \n",
    "    def calculate_gradient_norm(self):\n",
    "        \"\"\"Calculate total gradient norm for monitoring\"\"\"\n",
    "        total_norm = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        return total_norm ** 0.5\n",
    "    \n",
    "    def save_checkpoint(self, epoch, batch_idx, loss):\n",
    "        \"\"\"Save training checkpoint with proper state dict handling\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'batch_idx': batch_idx,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'loss': loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'test_losses': self.test_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'gradient_norms': self.gradient_norms\n",
    "        }\n",
    "        torch.save(checkpoint, 'training_checkpoint.pt')\n",
    "        print(f\"\\nCheckpoint saved at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load training checkpoint with proper state dict handling\"\"\"\n",
    "        if os.path.exists('training_checkpoint.pt'):\n",
    "            checkpoint = torch.load('training_checkpoint.pt', map_location=self.device)\n",
    "            \n",
    "            # Load model state\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # Load optimizer state\n",
    "            optimizer_state = checkpoint['optimizer_state_dict']\n",
    "            # Move optimizer state to correct device\n",
    "            for state in optimizer_state['state'].values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(self.device)\n",
    "            self.optimizer.load_state_dict(optimizer_state)\n",
    "            \n",
    "            # Load scheduler state if it exists\n",
    "            if checkpoint['scheduler_state_dict'] and self.scheduler:\n",
    "                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            \n",
    "            # Load training history\n",
    "            self.train_losses = checkpoint['train_losses']\n",
    "            self.test_losses = checkpoint['test_losses']\n",
    "            self.learning_rates = checkpoint['learning_rates']\n",
    "            self.gradient_norms = checkpoint['gradient_norms']\n",
    "            \n",
    "            start_epoch = checkpoint['epoch']\n",
    "            start_batch = checkpoint['batch_idx'] + 1\n",
    "            \n",
    "            print(f\"Resuming from epoch {start_epoch+1}, batch {start_batch}\")\n",
    "            return start_epoch, start_batch\n",
    "            \n",
    "        return 0, 0\n",
    "    \n",
    "    def train(self, epochs: int = 10, batch_size: int = 8):\n",
    "        \"\"\"\n",
    "        Train the model with checkpointing\n",
    "        \"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize or load scheduler\n",
    "        if self.scheduler is None:\n",
    "            self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                self.optimizer,\n",
    "                max_lr=1e-4,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                pct_start=0.3\n",
    "            )\n",
    "        \n",
    "        # Try to load checkpoint\n",
    "        start_epoch, start_batch = self.load_checkpoint()\n",
    "        \n",
    "        print(f\"Training starts with {epochs} epochs\")\n",
    "        print(f\"Total training steps per epoch: {len(train_loader)}\")\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(start_epoch, epochs):\n",
    "                self.model.train()\n",
    "                total_train_loss = 0\n",
    "                epoch_gradient_norms = []\n",
    "                avg_batch_loss = float('inf')\n",
    "                \n",
    "                pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "                \n",
    "                # Skip already processed batches if resuming\n",
    "                if epoch == start_epoch and start_batch > 0:\n",
    "                    for _ in range(start_batch):\n",
    "                        next(iter(pbar))\n",
    "                \n",
    "                for batch_idx, (texts, app_features, labels) in enumerate(pbar, start=start_batch):\n",
    "                    try:\n",
    "                        texts = texts.to(self.device)\n",
    "                        app_features = app_features.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        \n",
    "                        self.optimizer.zero_grad()\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = self.model(texts, app_features)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        \n",
    "                        # Update avg_batch_loss\n",
    "                        total_train_loss += loss.item()\n",
    "                        avg_batch_loss = total_train_loss / (batch_idx + 1)\n",
    "                        \n",
    "                        # Backward pass\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        # Gradient clipping\n",
    "                        grad_norm_before = self.calculate_gradient_norm()\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "                        grad_norm_after = self.calculate_gradient_norm()\n",
    "                        epoch_gradient_norms.append(grad_norm_after)\n",
    "                        \n",
    "                        # Gradient control\n",
    "                        for param in self.model.parameters():\n",
    "                            if param.grad is not None:\n",
    "                                torch.clamp_(param.grad, -1, 1)\n",
    "                        \n",
    "                        self.optimizer.step()\n",
    "                        self.scheduler.step()\n",
    "                        \n",
    "                        # Monitoring\n",
    "                        current_lr = self.scheduler.get_last_lr()[0]\n",
    "                        self.learning_rates.append(current_lr)\n",
    "                        \n",
    "                        pbar.set_postfix({\n",
    "                            'train_loss': f'{avg_batch_loss:.4f}',\n",
    "                            'lr': f'{current_lr:.6f}',\n",
    "                            'grad_norm': f'{grad_norm_after:.4f}'\n",
    "                        })\n",
    "                        \n",
    "                        # Save checkpoint every 5000 batches\n",
    "                        if (batch_idx + 1) % 5000 == 0:\n",
    "                            self.save_checkpoint(epoch, batch_idx, avg_batch_loss)\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                        # Save checkpoint on error\n",
    "                        self.save_checkpoint(epoch, max(0, batch_idx-1), avg_batch_loss)\n",
    "                        raise e\n",
    "                \n",
    "                # Calculate epoch metrics\n",
    "                avg_train_loss = total_train_loss / len(train_loader)\n",
    "                self.train_losses.append(avg_train_loss)\n",
    "                \n",
    "                # Evaluation phase\n",
    "                self.model.eval()\n",
    "                total_test_loss = 0\n",
    "                \n",
    "                print(\"\\nRunning evaluation...\")\n",
    "                with torch.no_grad():\n",
    "                    for texts, app_features, labels in test_loader:\n",
    "                        texts = texts.to(self.device)\n",
    "                        app_features = app_features.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(texts, app_features)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        total_test_loss += loss.item()\n",
    "                \n",
    "                avg_test_loss = total_test_loss / len(test_loader)\n",
    "                self.test_losses.append(avg_test_loss)\n",
    "                \n",
    "                # Print epoch summary\n",
    "                print(f'\\nEpoch {epoch+1}/{epochs} Summary:')\n",
    "                print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "                print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "                print(f'Learning Rate: {current_lr:.6f}')\n",
    "                print(f'Average Gradient Norm: {np.mean(epoch_gradient_norms):.4f}')\n",
    "                \n",
    "                # Save checkpoint at end of epoch\n",
    "                self.save_checkpoint(epoch, len(train_loader)-1, avg_train_loss)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Training interrupted: {str(e)}\")\n",
    "            print(\"You can resume training from the last checkpoint\")\n",
    "            raise e\n",
    "                \n",
    "            \n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=32, \n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, app_features, labels in test_loader:\n",
    "                texts = texts.to(self.device)\n",
    "                app_features = app_features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(texts, app_features)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                # move prediction result back to cpu\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        if self.device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
    "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
    "            'f1_score': f1_score(all_labels, all_preds, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "            'classification_report': classification_report(all_labels, all_preds)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training metrics history\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.test_losses, label='Test Loss')\n",
    "        plt.title('Loss History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot learning rates\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title('Learning Rate History')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        \n",
    "        # Plot gradient norms\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.gradient_norms)\n",
    "        plt.title('Gradient Norm History')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Gradient Norm')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "fd25834098831242",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:42.448383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_seed(507)\n",
    "analyzer = SentimentAnalyzer_new(new_df)\n",
    "try:\n",
    "    analyzer.train(epochs=30, batch_size=32)\n",
    "except Exception as e:\n",
    "    print(f\"Training interrupted: {e}\")\n",
    "        \n",
    "# Train the model\n",
    "analyzer.train(epochs=30)\n",
    "        \n",
    "# Evaluate the model\n",
    "metrics = analyzer.evaluate()\n",
    "        \n",
    "# Print metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")"
   ],
   "id": "20af7a6b6f6291a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
